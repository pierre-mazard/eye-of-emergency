# ğŸ¤– WORKFLOW D'ENTRAÃNEMENT ET TEST DES MODÃˆLES - EYE OF EMERGENCY

## ğŸ“‹ Vue d'ensemble

Ce guide dÃ©taille la marche Ã  suivre pour entraÃ®ner et tester diffÃ©rents modÃ¨les de machine learning sur le dataset Eye of Emergency optimisÃ© et validÃ©.

## ğŸ¯ Objectifs

- **EntraÃ®ner** : Plusieurs algorithmes de classification sur les tweets d'urgence
- **Comparer** : Les performances de diffÃ©rents modÃ¨les 
- **Optimiser** : Les hyperparamÃ¨tres pour chaque modÃ¨le
- **Valider** : La robustesse et la gÃ©nÃ©ralisation des modÃ¨les

---

## ğŸ“ Structure des fichiers pour l'entraÃ®nement

```
eye-of-emergency/
â”œâ”€â”€ data/processed/
â”‚   â”œâ”€â”€ train_optimized.csv     # âœ… Features engineering appliquÃ©
â”‚   â””â”€â”€ test_cleaned.csv        # âœ… DonnÃ©es propres pour test final
â”œâ”€â”€ results/models/             # ğŸ“‚ Sauvegarde des modÃ¨les entraÃ®nÃ©s
â”œâ”€â”€ results/figures/            # ğŸ“Š Graphiques et mÃ©triques
â””â”€â”€ src/models.py              # ğŸ§  Script des modÃ¨les (Ã  crÃ©er)
```

---

## ğŸš€ Ã‰TAPE 1 : PrÃ©paration des donnÃ©es

### âœ… PrÃ©requis - Pipeline terminÃ©
```bash
# S'assurer que le pipeline est exÃ©cutÃ©
python run_pipeline.py
```

**Fichiers gÃ©nÃ©rÃ©s requis :**
- `data/processed/train_optimized.csv` (6,185 tweets avec 17 features + metadata)
- `data/processed/test_cleaned.csv` (donnÃ©es test propres, 6 colonnes de base)

### ğŸ“Š Chargement et prÃ©paration
```python
import pandas as pd
from src.preprocess_train import OptimizedEmergencyPreprocessor

# Chargement des donnÃ©es d'entraÃ®nement
train_df = pd.read_csv('data/processed/train_optimized.csv')

# IMPORTANT: VÃ©rification des colonnes disponibles
print(f"ğŸ“‹ Colonnes dans train_optimized.csv: {list(train_df.columns)}")
print(f"ğŸ“Š Nombre total de colonnes: {len(train_df.columns)}")

# SÃ©paration features/target
# âœ… FEATURES NUMÃ‰RIQUES ET CATÃ‰GORIELLES : keyword + 16 features d'engineering
# âœ… ON GARDE : text_cleaned (utile pour analyse et dÃ©bogage)
# âŒ ON EXCLUT POUR ML : id (identifiant), target (variable cible)

# D'abord, sÃ©paration pour l'entraÃ®nement ML (features numÃ©riques/catÃ©gorielles uniquement)
X_train = train_df.drop(['id', 'target', 'text_cleaned'], axis=1)
y_train = train_df['target']

# Conserver text_cleaned sÃ©parÃ©ment pour analyse
text_train = train_df['text_cleaned']

print(f"ğŸ“Š Forme des donnÃ©es d'entraÃ®nement: {X_train.shape}")
print(f"ğŸ¯ Distribution des classes: {y_train.value_counts()}")
print(f"ğŸ”§ Features ML utilisÃ©es ({X_train.shape[1]}): {list(X_train.columns)}")
print(f"ğŸ“ Text_cleaned conservÃ© sÃ©parÃ©ment: {len(text_train)} textes")

# Validation que nous avons bien toutes les features ML
expected_features = 17  # keyword + 16 features d'engineering  
if X_train.shape[1] != expected_features:
    print(f"âš ï¸  ATTENTION: Expected {expected_features} features, got {X_train.shape[1]}")
else:
    print(f"âœ… Parfait! Nous utilisons bien les {expected_features} features ML attendues")
```

---

## ğŸ¤– Ã‰TAPE 2 : ModÃ¨les Ã  entraÃ®ner et comparer

### ğŸ—ï¸ Architecture recommandÃ©e

```python
# src/models.py
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import joblib

class EmergencyModelTrainer:
    def __init__(self):
        self.models = {
            'LogisticRegression': LogisticRegression(random_state=42),
            'RandomForest': RandomForestClassifier(random_state=42),
            'GradientBoosting': GradientBoostingClassifier(random_state=42),
            'XGBoost': XGBClassifier(random_state=42),
            'SVM': SVC(random_state=42),
            'NaiveBayes': MultinomialNB(),
            'KNN': KNeighborsClassifier()
        }
        self.best_models = {}
        self.results = {}
```

### ğŸ“ ModÃ¨les prioritaires
1. **Logistic Regression** - Baseline rapide et interprÃ©table
2. **Random Forest** - Robuste et gÃ¨re bien les features hÃ©tÃ©rogÃ¨nes
3. **Gradient Boosting** - Performances Ã©levÃ©es habituelles
4. **XGBoost** - Ã‰tat de l'art pour ce type de donnÃ©es
5. **SVM** - Bon pour la classification de texte
6. **Naive Bayes** - Rapide, bon pour le texte
7. **KNN** - Comparaison locale

---

## âš™ï¸ GESTION DES FEATURES : DIFFÃ‰RENCE TRAIN vs TEST

### ğŸ“Š Comprendre la structure des donnÃ©es

**ğŸ”§ train_optimized.csv (20 colonnes) :**
```
id, keyword, target, text_cleaned, 
text_length, word_count, char_count, has_emergency_word, 
emergency_word_count, emergency_density, has_url, url_count, 
has_mention, mention_count, exclamation_count, intense_punctuation, 
avg_word_length, urgency_score, stopword_ratio, keyword_in_text
```

**ğŸ§½ test_cleaned.csv (6 colonnes) :**
```
id, keyword, location, text, target, text_cleaned
```

**âš ï¸ Important pour le test :**
- **`text`** : Texte original (non utilisÃ© pour ML)
- **`text_cleaned`** : Texte nettoyÃ© (utilisÃ© pour feature engineering)
- **`location`** : Information gÃ©ographique (non utilisÃ©e pour ML)
- **`keyword`** : Mot-clÃ© (utilisÃ© comme feature)
- **`target`** : Labels (pour validation finale)

### ğŸ¯ StratÃ©gie de feature engineering

1. **TRAIN** : Features dÃ©jÃ  calculÃ©es et stockÃ©es (17 features ML + text_cleaned conservÃ©)
2. **TEST** : Features calculÃ©es en temps rÃ©el lors de la prÃ©diction  
3. **COHÃ‰RENCE** : MÃªmes 17 features ML pour train et test
4. **ANALYSE** : text_cleaned disponible pour debugging et interprÃ©tation

### âš ï¸ DiffÃ©rences importantes TRAIN vs TEST

**TRAIN (train_optimized.csv) :**
- âœ… Features dÃ©jÃ  calculÃ©es et sauvegardÃ©es
- âœ… Utilise `text_cleaned` pour l'analyse
- âœ… PrÃªt pour l'entraÃ®nement direct

**TEST (test_cleaned.csv) :**
- âŒ Pas de features d'engineering prÃ©-calculÃ©es  
- âœ… Utilise `text_cleaned` pour gÃ©nÃ©rer les features en temps rÃ©el
- âœ… Contient `location` (non utilisÃ©e pour ML)
- âš ï¸ **IMPORTANT** : Ne jamais utiliser `text` (raw), toujours `text_cleaned`

### âœ… Features ML utilisÃ©es (17 au total)

**Pour l'entraÃ®nement des modÃ¨les :**

1. **keyword** (categorical)
2. **text_length** (numeric) 
3. **word_count** (numeric)
4. **char_count** (numeric)
5. **has_emergency_word** (boolean)
6. **emergency_word_count** (numeric)
7. **emergency_density** (numeric)
8. **has_url** (boolean)
9. **url_count** (numeric)
10. **has_mention** (boolean)
11. **mention_count** (numeric)
12. **exclamation_count** (numeric)
13. **intense_punctuation** (numeric)
14. **avg_word_length** (numeric)
15. **urgency_score** (numeric)
16. **stopword_ratio** (numeric)
17. **keyword_in_text** (boolean)

### ğŸ“ DonnÃ©es additionnelles conservÃ©es

**text_cleaned** : Texte prÃ©processÃ© conservÃ© sÃ©parÃ©ment pour :
- ğŸ” **Analyse des erreurs** : Comprendre pourquoi un modÃ¨le se trompe
- ğŸ› **DÃ©bogage** : VÃ©rifier la qualitÃ© du preprocessing  
- ğŸ“Š **InterprÃ©tation** : Analyser les patterns dans les prÃ©dictions
- ğŸ“ˆ **Visualisation** : CrÃ©er des word clouds et analyses textuelles
- ğŸ§ª **Validation** : VÃ©rifier la cohÃ©rence des features extraites

```python
# Exemple d'usage de text_cleaned pour analyse
def analyze_predictions(y_true, y_pred, text_cleaned, X_train):
    """Analyse des prÃ©dictions avec le texte original"""
    
    # Identifier les erreurs
    errors = y_true != y_pred
    error_texts = text_cleaned[errors]
    error_features = X_train[errors]
    
    print(f"ğŸ“Š Nombre d'erreurs: {errors.sum()}")
    print(f"ğŸ“ Exemples de textes mal classÃ©s:")
    for i, text in enumerate(error_texts.head()):
        print(f"   {i+1}. {text}")
    
    return error_texts, error_features
```

---

## âš™ï¸ Ã‰TAPE 3 : EntraÃ®nement et optimisation

### ğŸ”„ Cross-validation et grid search
```python
def train_and_optimize_models(X_train, y_train):
    """EntraÃ®ne et optimise tous les modÃ¨les"""
    
    # HyperparamÃ¨tres Ã  tester
    param_grids = {
        'LogisticRegression': {
            'C': [0.1, 1, 10, 100],
            'max_iter': [1000, 2000]
        },
        'RandomForest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10]
        },
        'GradientBoosting': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        },
        'XGBoost': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        }
    }
    
    results = {}
    for model_name, model in models.items():
        print(f"ğŸ”„ EntraÃ®nement {model_name}...")
        
        if model_name in param_grids:
            # Grid search avec cross-validation
            grid_search = GridSearchCV(
                model, 
                param_grids[model_name],
                cv=5,
                scoring='f1_macro',
                n_jobs=-1
            )
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            best_score = grid_search.best_score_
        else:
            # Cross-validation simple
            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro')
            model.fit(X_train, y_train)
            best_model = model
            best_score = scores.mean()
        
        results[model_name] = {
            'model': best_model,
            'cv_score': best_score,
            'std': scores.std() if 'scores' in locals() else 0
        }
        
        # Sauvegarde du modÃ¨le
        joblib.dump(best_model, f'results/models/{model_name}_best.pkl')
        print(f"âœ… {model_name}: CV Score = {best_score:.4f}")
    
    return results
```

---

## ğŸ“Š Ã‰TAPE 4 : Ã‰valuation et comparaison

### ğŸ† MÃ©triques de performance
```python
def evaluate_models(results, X_train, y_train):
    """Ã‰value et compare tous les modÃ¨les entraÃ®nÃ©s"""
    
    evaluation_results = []
    
    for model_name, result in results.items():
        model = result['model']
        
        # PrÃ©dictions sur train (pour dÃ©tecter overfitting)
        y_pred_train = model.predict(X_train)
        
        # MÃ©triques dÃ©taillÃ©es
        from sklearn.metrics import precision_recall_fscore_support, accuracy_score
        
        accuracy = accuracy_score(y_train, y_pred_train)
        precision, recall, f1, _ = precision_recall_fscore_support(y_train, y_pred_train, average='macro')
        
        evaluation_results.append({
            'Model': model_name,
            'CV_Score': result['cv_score'],
            'CV_Std': result['std'],
            'Train_Accuracy': accuracy,
            'Train_Precision': precision,
            'Train_Recall': recall,
            'Train_F1': f1
        })
    
    # DataFrame des rÃ©sultats
    results_df = pd.DataFrame(evaluation_results)
    results_df = results_df.sort_values('CV_Score', ascending=False)
    
    print("ğŸ† CLASSEMENT DES MODÃˆLES:")
    print(results_df.to_string(index=False))
    
    # Sauvegarde
    results_df.to_csv('results/models_comparison.csv', index=False)
    
    return results_df

def analyze_model_errors(best_model, X_train, y_train, text_train, model_name):
    """Analyse dÃ©taillÃ©e des erreurs avec text_cleaned"""
    
    y_pred = best_model.predict(X_train)
    errors = y_train != y_pred
    
    print(f"\nğŸ” ANALYSE DES ERREURS - {model_name}")
    print(f"ğŸ“Š Nombre total d'erreurs: {errors.sum()}/{len(y_train)} ({errors.mean()*100:.2f}%)")
    
    if errors.sum() > 0:
        print(f"\nğŸ“ Exemples de textes mal classÃ©s:")
        error_indices = errors[errors].index[:5]  # 5 premiers exemples
        
        for i, idx in enumerate(error_indices):
            true_label = y_train.iloc[idx]
            pred_label = y_pred[idx]
            text = text_train.iloc[idx]
            
            print(f"\n   {i+1}. Vrai: {true_label}, PrÃ©dit: {pred_label}")
            print(f"      Texte: {text[:100]}...")
    
    return errors
```

### ğŸ“ˆ Visualisations de performance
```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_model_comparison(results_df):
    """GÃ©nÃ¨re les graphiques de comparaison"""
    
    # 1. Comparaison CV Score
    plt.figure(figsize=(12, 6))
    sns.barplot(data=results_df, x='Model', y='CV_Score')
    plt.title('ğŸ† Comparaison des performances (Cross-Validation F1-Score)')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('results/figures/models_cv_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. Train vs CV Score (dÃ©tection overfitting)
    plt.figure(figsize=(10, 6))
    plt.scatter(results_df['CV_Score'], results_df['Train_F1'], s=100)
    for i, model in enumerate(results_df['Model']):
        plt.annotate(model, (results_df['CV_Score'].iloc[i], results_df['Train_F1'].iloc[i]))
    plt.plot([0, 1], [0, 1], 'r--', alpha=0.5)
    plt.xlabel('CV F1-Score')
    plt.ylabel('Train F1-Score')
    plt.title('ğŸ¯ DÃ©tection d\'overfitting : Train vs CV Performance')
    plt.tight_layout()
    plt.savefig('results/figures/overfitting_detection.png', dpi=300, bbox_inches='tight')
    plt.show()
```

---

## ğŸ§ª Ã‰TAPE 5 : Test final sur donnÃ©es test

### âš ï¸ Important : Feature engineering en temps rÃ©el sur test
```python
def test_final_model(best_model_name, test_file='data/processed/test_cleaned.csv'):
    """Test final du meilleur modÃ¨le sur donnÃ©es test"""
    
    # Chargement du meilleur modÃ¨le
    best_model = joblib.load(f'results/models/{best_model_name}_best.pkl')
    
    # Ã‰TAPE 1: Chargement donnÃ©es test (propres, sans feature engineering)
    test_df = pd.read_csv(test_file)
    print(f"ğŸ“Š Test data shape: {test_df.shape}")
    print(f"ğŸ“‹ Test columns: {list(test_df.columns)}")
    
    # VALIDATION: VÃ©rifier que les colonnes attendues sont prÃ©sentes
    required_columns = ['id', 'keyword', 'text_cleaned']
    missing_columns = [col for col in required_columns if col not in test_df.columns]
    
    if missing_columns:
        print(f"âŒ ERREUR: Colonnes manquantes dans le test: {missing_columns}")
        print(f"   Colonnes disponibles: {list(test_df.columns)}")
        return None
    
    print(f"âœ… Colonnes requises prÃ©sentes: {required_columns}")
    
    # VÃ©rifier la qualitÃ© des donnÃ©es
    empty_texts = test_df['text_cleaned'].isna().sum()
    if empty_texts > 0:
        print(f"âš ï¸  {empty_texts} textes vides dans text_cleaned")
    else:
        print(f"âœ… Tous les {len(test_df)} textes sont disponibles")
    
    # Ã‰TAPE 2: Feature engineering en temps rÃ©el sur test
    # CRITIQUE: Le test n'a que 6 colonnes de base, nous devons gÃ©nÃ©rer les 17 features
    # On utilise 'text_cleaned' (pas 'text') car c'est le texte preprocessÃ©
    preprocessor = OptimizedEmergencyPreprocessor()
    
    test_features = []
    print("ğŸ”„ Application du feature engineering en temps rÃ©el...")
    
    for idx, row in test_df.iterrows():
        # IMPORTANT: Utiliser text_cleaned (pas text) et keyword du fichier test
        # Structure test_cleaned.csv: id, keyword, location, text, target, text_cleaned
        
        # Extraction des 16 features d'engineering Ã  partir du texte NETTOYÃ‰
        engineered_features = preprocessor.extract_optimized_features(row['text_cleaned'])
        
        # Ajout du keyword comme feature (pour correspondre au train)
        full_features = {'keyword': row['keyword']}
        full_features.update(engineered_features)
        
        test_features.append(full_features)
        
        if idx % 500 == 0:
            print(f"   Processed {idx+1}/{len(test_df)} samples...")
    
    # Ã‰TAPE 3: CrÃ©ation du DataFrame avec les mÃªmes colonnes que X_train
    X_test = pd.DataFrame(test_features)
    
    # VALIDATION CRITIQUE: VÃ©rifier que les colonnes correspondent exactement
    # Charger X_train pour comparaison
    train_df = pd.read_csv('data/processed/train_optimized.csv')
    X_train_columns = train_df.drop(['id', 'target', 'text_cleaned'], axis=1).columns.tolist()
    
    print(f"ğŸ” VALIDATION DES FEATURES:")
    print(f"   Train features ({len(X_train_columns)}): {X_train_columns}")
    print(f"   Test features ({len(X_test.columns)}): {list(X_test.columns)}")
    
    # RÃ©organiser les colonnes de test pour correspondre exactement Ã  train
    X_test = X_test[X_train_columns]
    
    if list(X_test.columns) == X_train_columns:
        print("âœ… PARFAIT! Les features test correspondent exactement au train")
    else:
        print("âŒ ERREUR! Mismatch entre features train et test")
        return None
    
    print(f"ğŸ“Š Forme finale X_test: {X_test.shape}")
    
    # Ã‰TAPE 4: PrÃ©dictions finales
    y_pred_test = best_model.predict(X_test)
    y_proba_test = best_model.predict_proba(X_test)
    
    # Sauvegarde des prÃ©dictions
    test_predictions = test_df.copy()
    test_predictions['predicted_emergency'] = y_pred_test
    test_predictions['probability_emergency'] = y_proba_test[:, 1]
    
    test_predictions.to_csv('results/test_predictions.csv', index=False)
    
    print(f"âœ… PrÃ©dictions test sauvegardÃ©es dans results/test_predictions.csv")
    print(f"ğŸ“Š Distribution des prÃ©dictions: {pd.Series(y_pred_test).value_counts()}")
    
    return test_predictions
```

---

## ğŸ“‹ Ã‰TAPE 6 : Workflow complet d'exÃ©cution

### ğŸš€ Script principal recommandÃ©
```bash
# 1. S'assurer que le pipeline est terminÃ©
python run_pipeline.py

# 2. CrÃ©er le script de modÃ¨les
# Copier le code ci-dessus dans src/models.py

# 3. EntraÃ®ner tous les modÃ¨les
python -c "
from src.models import *
import pandas as pd

# Chargement donnÃ©es avec validation complÃ¨te
train_df = pd.read_csv('data/processed/train_optimized.csv')
print(f'ğŸ“‹ Train columns: {list(train_df.columns)}')

# SÃ©paration features/target (features ML uniquement)
X_train = train_df.drop(['id', 'target', 'text_cleaned'], axis=1)
y_train = train_df['target']
text_train = train_df['text_cleaned']  # ConservÃ© pour analyse

print(f'âœ… Features ML shape: {X_train.shape}')
print(f'ğŸ“Š Features ML list: {list(X_train.columns)}')
print(f'ğŸ“ Text data conservÃ©: {len(text_train)} textes')

# EntraÃ®nement
trainer = EmergencyModelTrainer()
results = trainer.train_and_optimize_models(X_train, y_train)

# Ã‰valuation
eval_results = trainer.evaluate_models(results, X_train, y_train)
trainer.plot_model_comparison(eval_results)

# Analyse d'erreurs du meilleur modÃ¨le
best_model_name = eval_results.iloc[0]['Model']
best_model = results[best_model_name]['model']
trainer.analyze_model_errors(best_model, X_train, y_train, text_train, best_model_name)

print('ğŸ‰ EntraÃ®nement et analyse terminÃ©s!')
"

# 4. Test final du meilleur modÃ¨le
python -c "
from src.models import test_final_model
test_final_model('XGBoost')  # Remplacer par le meilleur modÃ¨le
"
```

---

## ğŸ“Š Livrables attendus

### ğŸ“ Fichiers gÃ©nÃ©rÃ©s
```
results/
â”œâ”€â”€ models/                          # ğŸ¤– ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ LogisticRegression_best.pkl
â”‚   â”œâ”€â”€ RandomForest_best.pkl
â”‚   â”œâ”€â”€ XGBoost_best.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ figures/                         # ğŸ“ˆ Visualisations
â”‚   â”œâ”€â”€ models_cv_comparison.png
â”‚   â”œâ”€â”€ overfitting_detection.png
â”‚   â””â”€â”€ confusion_matrices/
â”œâ”€â”€ models_comparison.csv            # ğŸ“‹ Comparaison dÃ©taillÃ©e
â””â”€â”€ test_predictions.csv             # ğŸ¯ PrÃ©dictions finales
```

### ğŸ“ˆ MÃ©triques clÃ©s Ã  analyser
1. **F1-Score macro** (mÃ©trique principale)
2. **PrÃ©cision/Rappel** par classe
3. **Accuracy** globale  
4. **Temps d'entraÃ®nement**
5. **Robustesse** (Ã©cart-type CV)
6. **Overfitting** (Ã©cart train/validation)

---

## âœ… Checklist de validation

- [ ] Pipeline de donnÃ©es exÃ©cutÃ© avec succÃ¨s
- [ ] 7 modÃ¨les diffÃ©rents entraÃ®nÃ©s et optimisÃ©s
- [ ] Cross-validation 5-fold sur tous les modÃ¨les
- [ ] HyperparamÃ¨tres optimisÃ©s par grid search
- [ ] MÃ©triques de performance calculÃ©es et sauvegardÃ©es
- [ ] Visualisations de comparaison gÃ©nÃ©rÃ©es
- [ ] DÃ©tection d'overfitting effectuÃ©e
- [ ] Test final sur donnÃ©es test avec feature engineering temps rÃ©el
- [ ] PrÃ©dictions finales sauvegardÃ©es
- [ ] Documentation des rÃ©sultats complÃ¨te

---

## ğŸ”® Prochaines Ã©tapes suggÃ©rÃ©es

1. **Ensemble Methods** : Combiner les meilleurs modÃ¨les
2. **Feature Selection** : Identifier les features les plus importantes
3. **ExplicabilitÃ©** : SHAP values pour interprÃ©ter les prÃ©dictions
4. **DÃ©ploiement** : API REST pour prÃ©dictions en temps rÃ©el
5. **Monitoring** : Surveillance de la dÃ©rive des donnÃ©es

---

**ğŸ“ Note importante** : Ce workflow garantit l'intÃ©gritÃ© des donnÃ©es en appliquant le feature engineering uniquement au moment des prÃ©dictions sur les donnÃ©es test, Ã©vitant ainsi toute fuite de donnÃ©es.
