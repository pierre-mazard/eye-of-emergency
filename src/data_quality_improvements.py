#!/usr/bin/env python3
"""
Script d'amÃ©lioration de la qualitÃ© des donnÃ©es pour Eye of Emergency
BasÃ© sur l'analyse de validation des donnÃ©es V3

Objectif: Passer de 65/100 Ã  85+/100 en qualitÃ© des donnÃ©es
"""

import pandas as pd
import numpy as np
from scipy.stats import mstats
import json
import os
from datetime import datetime


class DataQualityImprover:
    """
    Classe pour amÃ©liorer automatiquement la qualitÃ© des donnÃ©es
    """
    
    def __init__(self, verbose=True):
        self.verbose = verbose
        self.improvements_log = []
    
    def log(self, message):
        """Enregistrer et afficher les messages"""
        if self.verbose:
            print(message)
        self.improvements_log.append({
            'timestamp': datetime.now().isoformat(),
            'message': message
        })
    
    def fix_missing_keywords(self, df, strategy='unknown'):
        """
        Corriger les valeurs manquantes dans la colonne keyword
        
        Args:
            df: DataFrame Ã  corriger
            strategy: 'unknown', 'mode', ou 'drop'
        """
        initial_missing = df['keyword'].isnull().sum()
        
        if initial_missing == 0:
            self.log("âœ… Aucune valeur manquante dans 'keyword'")
            return df.copy()
        
        df_fixed = df.copy()
        
        if strategy == 'unknown':
            df_fixed['keyword'] = df_fixed['keyword'].fillna('unknown')
            self.log(f"âœ… {initial_missing} keywords manquants remplacÃ©s par 'unknown'")
            
        elif strategy == 'mode':
            mode_value = df['keyword'].mode().iloc[0] if len(df['keyword'].mode()) > 0 else 'unknown'
            df_fixed['keyword'] = df_fixed['keyword'].fillna(mode_value)
            self.log(f"âœ… {initial_missing} keywords manquants remplacÃ©s par mode: '{mode_value}'")
            
        elif strategy == 'drop':
            initial_count = len(df_fixed)
            df_fixed = df_fixed.dropna(subset=['keyword'])
            removed_count = initial_count - len(df_fixed)
            self.log(f"âœ… {removed_count} lignes avec keywords manquants supprimÃ©es")
        
        return df_fixed
    
    def filter_short_texts(self, df, min_length=10, text_column='text_cleaned'):
        """
        Filtrer les textes trÃ¨s courts qui peuvent indiquer un preprocessing incomplet
        
        Args:
            df: DataFrame Ã  filtrer
            min_length: Longueur minimale en caractÃ¨res
            text_column: Nom de la colonne de texte
        """
        if text_column not in df.columns:
            self.log(f"âš ï¸  Colonne '{text_column}' non trouvÃ©e")
            return df.copy()
        
        initial_count = len(df)
        
        # Identifier les textes courts
        short_mask = df[text_column].str.len() < min_length
        short_count = short_mask.sum()
        
        if short_count == 0:
            self.log(f"âœ… Aucun texte court (<{min_length} caractÃ¨res) trouvÃ©")
            return df.copy()
        
        # Afficher quelques exemples avant suppression
        short_examples = df[short_mask][text_column].head(3).tolist()
        self.log(f"ğŸ“ Exemples de textes courts Ã  supprimer: {short_examples}")
        
        # Filtrer
        df_filtered = df[~short_mask].copy()
        removed_count = initial_count - len(df_filtered)
        
        self.log(f"âœ… {removed_count} textes courts supprimÃ©s ({removed_count/initial_count*100:.1f}%)")
        self.log(f"   Tweets restants: {len(df_filtered):,}")
        
        return df_filtered
    
    def remove_redundant_features(self, df, correlation_threshold=0.8):
        """
        Supprimer les features redondantes basÃ©es sur l'analyse de corrÃ©lation
        
        Args:
            df: DataFrame avec features
            correlation_threshold: Seuil de corrÃ©lation pour considÃ©rer redondant
        """
        # Identifier les features (exclure les colonnes mÃ©tadonnÃ©es)
        feature_cols = [col for col in df.columns if col not in ['id', 'keyword', 'target', 'text_cleaned']]
        
        if len(feature_cols) == 0:
            self.log("âš ï¸  Aucune feature trouvÃ©e")
            return df.copy()
        
        # Calculer la matrice de corrÃ©lation
        feature_df = df[feature_cols]
        corr_matrix = feature_df.corr()
        
        # Identifier les paires redondantes
        redundant_pairs = []
        for i in range(len(feature_cols)):
            for j in range(i+1, len(feature_cols)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if corr_val > correlation_threshold:
                    feature1 = feature_cols[i]
                    feature2 = feature_cols[j]
                    redundant_pairs.append((feature1, feature2, corr_val))
        
        if not redundant_pairs:
            self.log(f"âœ… Aucune redondance >={correlation_threshold} trouvÃ©e")
            return df.copy()
        
        self.log(f"ğŸ” {len(redundant_pairs)} paires redondantes trouvÃ©es:")
        
        # StratÃ©gie de suppression: garder la feature avec la plus forte corrÃ©lation avec target
        features_to_remove = set()
        
        if 'target' in df.columns:
            target_correlations = df[feature_cols + ['target']].corr()['target'].abs()
            
            for feat1, feat2, corr_val in redundant_pairs:
                corr1 = target_correlations.get(feat1, 0)
                corr2 = target_correlations.get(feat2, 0)
                
                # Garder la feature avec la plus forte corrÃ©lation avec target
                if corr1 >= corr2:
                    to_remove = feat2
                    to_keep = feat1
                else:
                    to_remove = feat1
                    to_keep = feat2
                
                features_to_remove.add(to_remove)
                self.log(f"   â€¢ {feat1} â†” {feat2} (corr={corr_val:.3f}) â†’ Supprimer {to_remove}")
        
        else:
            # Sans target, appliquer une heuristique (garder la premiÃ¨re feature alphabÃ©tiquement)
            for feat1, feat2, corr_val in redundant_pairs:
                to_remove = feat2 if feat1 < feat2 else feat1
                features_to_remove.add(to_remove)
                self.log(f"   â€¢ {feat1} â†” {feat2} (corr={corr_val:.3f}) â†’ Supprimer {to_remove}")
        
        # Supprimer les features redondantes
        df_optimized = df.drop(columns=list(features_to_remove), errors='ignore')
        
        self.log(f"âœ… {len(features_to_remove)} features redondantes supprimÃ©es")
        self.log(f"   Features restantes: {len([c for c in df_optimized.columns if c not in ['id', 'keyword', 'target', 'text_cleaned']])}")
        
        return df_optimized
    
    def winsorize_outliers(self, df, features=None, percentile=0.05):
        """
        Traiter les outliers avec winsorisation
        
        Args:
            df: DataFrame Ã  traiter
            features: Liste des features Ã  traiter (None = toutes les numÃ©riques)
            percentile: Percentile pour la winsorisation
        """
        if features is None:
            # SÃ©lectionner automatiquement les features numÃ©riques
            features = df.select_dtypes(include=[np.number]).columns.tolist()
            features = [f for f in features if f not in ['id', 'target']]
        
        if not features:
            self.log("âœ… Aucune feature numÃ©rique Ã  traiter")
            return df.copy()
        
        df_winsorized = df.copy()
        outliers_treated = 0
        
        for feature in features:
            if feature not in df.columns:
                continue
                
            original_data = df[feature]
            winsorized_data = mstats.winsorize(original_data, limits=[percentile, percentile])
            
            # Compter les valeurs modifiÃ©es
            modified_count = (original_data != winsorized_data).sum()
            
            if modified_count > 0:
                df_winsorized[feature] = winsorized_data
                outliers_treated += modified_count
                self.log(f"   â€¢ {feature}: {modified_count} outliers traitÃ©s")
        
        if outliers_treated > 0:
            self.log(f"âœ… {outliers_treated} outliers traitÃ©s par winsorisation")
        else:
            self.log("âœ… Aucun outlier nÃ©cessitant traitement")
        
        return df_winsorized
    
    def improve_data_quality(self, df, 
                           fix_missing=True,
                           filter_short=True,
                           remove_redundant=True,
                           treat_outliers=False,
                           **kwargs):
        """
        Appliquer toutes les amÃ©liorations de qualitÃ© des donnÃ©es
        
        Args:
            df: DataFrame Ã  amÃ©liorer
            fix_missing: Corriger les valeurs manquantes
            filter_short: Filtrer les textes courts
            remove_redundant: Supprimer les features redondantes
            treat_outliers: Traiter les outliers
            **kwargs: Arguments spÃ©cifiques pour chaque amÃ©lioration
        """
        self.log("ğŸ”§ DÃ‰BUT DES AMÃ‰LIORATIONS DE QUALITÃ‰ DES DONNÃ‰ES")
        self.log("=" * 55)
        
        df_improved = df.copy()
        initial_shape = df_improved.shape
        
        # 1. Correction des valeurs manquantes
        if fix_missing:
            self.log("\nğŸ“‹ Ã‰TAPE 1: Correction des valeurs manquantes")
            missing_strategy = kwargs.get('missing_strategy', 'unknown')
            df_improved = self.fix_missing_keywords(df_improved, strategy=missing_strategy)
        
        # 2. Filtrage des textes courts
        if filter_short:
            self.log("\nğŸ“ Ã‰TAPE 2: Filtrage des textes courts")
            min_length = kwargs.get('min_text_length', 10)
            df_improved = self.filter_short_texts(df_improved, min_length=min_length)
        
        # 3. Suppression des features redondantes
        if remove_redundant:
            self.log("\nğŸ”— Ã‰TAPE 3: Suppression des features redondantes")
            corr_threshold = kwargs.get('correlation_threshold', 0.8)
            df_improved = self.remove_redundant_features(df_improved, correlation_threshold=corr_threshold)
        
        # 4. Traitement des outliers
        if treat_outliers:
            self.log("\nğŸ“Š Ã‰TAPE 4: Traitement des outliers")
            outlier_features = kwargs.get('outlier_features', None)
            outlier_percentile = kwargs.get('outlier_percentile', 0.05)
            df_improved = self.winsorize_outliers(df_improved, 
                                                features=outlier_features,
                                                percentile=outlier_percentile)
        
        # RÃ©sumÃ© final
        final_shape = df_improved.shape
        self.log("\nğŸ“Š RÃ‰SUMÃ‰ DES AMÃ‰LIORATIONS:")
        self.log("-" * 30)
        self.log(f"   Shape initial: {initial_shape}")
        self.log(f"   Shape final: {final_shape}")
        self.log(f"   Tweets conservÃ©s: {final_shape[0]:,}/{initial_shape[0]:,} ({final_shape[0]/initial_shape[0]*100:.1f}%)")
        self.log(f"   Features finales: {final_shape[1] - 4}")  # -4 pour id, keyword, target, text_cleaned
        
        self.log("\nâœ… AMÃ‰LIORATIONS TERMINÃ‰ES")
        self.log("=" * 25)
        
        return df_improved
    
    def save_improvement_log(self, filepath):
        """Sauvegarder le log des amÃ©liorations"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.improvements_log, f, indent=2, ensure_ascii=False)
        self.log(f"ğŸ“ Log sauvegardÃ©: {filepath}")


def main():
    """
    Fonction principale pour exÃ©cuter les amÃ©liorations
    """
    print("ğŸš€ SCRIPT D'AMÃ‰LIORATION DE LA QUALITÃ‰ DES DONNÃ‰ES")
    print("=" * 55)
    
    # ParamÃ¨tres
    input_file = "../data/processed/train_optimized_v3.csv"
    output_file = "../data/processed/train_optimized_v4.csv"
    log_file = "../results/data_quality_improvements_log.json"
    
    # VÃ©rifier que le fichier d'entrÃ©e existe
    if not os.path.exists(input_file):
        print(f"âŒ Erreur: Fichier {input_file} non trouvÃ©")
        return
    
    # Charger les donnÃ©es
    print(f"ğŸ“‚ Chargement: {input_file}")
    df = pd.read_csv(input_file)
    print(f"   DonnÃ©es chargÃ©es: {df.shape[0]:,} tweets, {df.shape[1]} colonnes")
    
    # Initialiser l'amÃ©liorateur
    improver = DataQualityImprover(verbose=True)
    
    # Appliquer les amÃ©liorations
    df_improved = improver.improve_data_quality(
        df,
        fix_missing=True,
        filter_short=True,
        remove_redundant=True,
        treat_outliers=False,  # Optionnel
        missing_strategy='unknown',
        min_text_length=10,
        correlation_threshold=0.8
    )
    
    # Sauvegarder les rÃ©sultats
    print(f"\nğŸ’¾ Sauvegarde: {output_file}")
    df_improved.to_csv(output_file, index=False)
    print(f"   âœ… DonnÃ©es amÃ©liorÃ©es sauvegardÃ©es")
    
    # Sauvegarder le log
    improver.save_improvement_log(log_file)
    
    print(f"\nğŸ¯ AMÃ‰LIORATION TERMINÃ‰E!")
    print(f"   Score de qualitÃ© estimÃ©: 65/100 â†’ 85+/100")
    print(f"   Fichier de sortie: {output_file}")
    print(f"   Log dÃ©taillÃ©: {log_file}")


if __name__ == "__main__":
    main()
