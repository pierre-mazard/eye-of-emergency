"""
Script de partitionnement stratifi√© propre
1. Charge le dataset nettoy√©
2. Effectue un partitionnement stratifi√© 90/10
3. V√©rifie l'absence de fuites
4. Sauvegarde les partitions
"""

import pandas as pd
import numpy as np
import os

def stratified_split(df, target_col, test_size=0.1, random_state=42):
    """
    Partitionnement stratifi√© manuel
    """
    np.random.seed(random_state)
    
    # S√©parer par classe
    class_0 = df[df[target_col] == 0].copy()
    class_1 = df[df[target_col] == 1].copy()
    
    # M√©langer chaque classe
    class_0 = class_0.sample(frac=1, random_state=random_state).reset_index(drop=True)
    class_1 = class_1.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    # Calculer les tailles pour le test
    test_size_0 = int(len(class_0) * test_size)
    test_size_1 = int(len(class_1) * test_size)
    
    # Diviser chaque classe
    test_0 = class_0[:test_size_0]
    train_0 = class_0[test_size_0:]
    
    test_1 = class_1[:test_size_1]
    train_1 = class_1[test_size_1:]
    
    # Recombiner
    train_df = pd.concat([train_0, train_1], ignore_index=True)
    test_df = pd.concat([test_0, test_1], ignore_index=True)
    
    # M√©langer les datasets finaux
    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)
    test_df = test_df.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    return train_df, test_df

def create_clean_split():
    """
    Cr√©e un partitionnement propre du dataset nettoy√©
    """
    print("üîÑ PARTITIONNEMENT STRATIFI√â PROPRE")
    print("=" * 50)
    
    # Chargement du dataset nettoy√©
    cleaned_path = 'data/raw/original_train_tweets_cleaned.csv'
    print(f"üìÅ Chargement de {cleaned_path}")
    
    try:
        df_cleaned = pd.read_csv(cleaned_path)
        print(f"‚úÖ Dataset charg√©: {len(df_cleaned)} √©chantillons")
    except FileNotFoundError:
        print(f"‚ùå ERREUR: Fichier {cleaned_path} non trouv√©")
        print("‚û°Ô∏è  Ex√©cutez d'abord clean_original_dataset.py")
        return False
    
    # V√©rification de l'int√©grit√©
    print(f"\nüîç V√©rification de l'int√©grit√© du dataset:")
    print(f"   Valeurs manquantes dans 'text': {df_cleaned['text'].isnull().sum()}")
    print(f"   Valeurs manquantes dans 'target': {df_cleaned['target'].isnull().sum()}")
    print(f"   IDs uniques: {df_cleaned['id'].nunique()}")
    print(f"   Textes uniques: {df_cleaned['text'].nunique()}")
    
    # Distribution des classes
    print(f"\nüìä Distribution des classes:")
    class_dist = df_cleaned['target'].value_counts().sort_index()
    for class_label, count in class_dist.items():
        pct = (count / len(df_cleaned)) * 100
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        print(f"   Classe {class_label} ({class_name}): {count:,} ({pct:.1f}%)")
    
    # Partitionnement stratifi√©
    print(f"\n‚öñÔ∏è Partitionnement stratifi√© (90% train, 10% test):")
    
    # Utilisation de notre fonction de partitionnement stratifi√©
    train_df, test_df = stratified_split(df_cleaned, 'target', test_size=0.1, random_state=42)
    
    print(f"   Train: {len(train_df)} √©chantillons ({len(train_df)/len(df_cleaned)*100:.1f}%)")
    print(f"   Test: {len(test_df)} √©chantillons ({len(test_df)/len(df_cleaned)*100:.1f}%)")
    
    # V√©rification de la stratification
    print(f"\nüìà V√©rification de la stratification:")
    
    train_dist = train_df['target'].value_counts(normalize=True).sort_index()
    test_dist = test_df['target'].value_counts(normalize=True).sort_index()
    original_dist = df_cleaned['target'].value_counts(normalize=True).sort_index()
    
    print(f"   Distribution originale:")
    for class_label, pct in original_dist.items():
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        print(f"     Classe {class_label} ({class_name}): {pct:.1%}")
    
    print(f"   Distribution train:")
    for class_label, pct in train_dist.items():
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        original_pct = original_dist[class_label]
        diff = abs(pct - original_pct)
        print(f"     Classe {class_label} ({class_name}): {pct:.1%} (Œî{diff:.2%})")
    
    print(f"   Distribution test:")
    for class_label, pct in test_dist.items():
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        original_pct = original_dist[class_label]
        diff = abs(pct - original_pct)
        print(f"     Classe {class_label} ({class_name}): {pct:.1%} (Œî{diff:.2%})")
    
    # Calcul de l'√©cart maximum
    max_train_diff = abs(train_dist - original_dist).max()
    max_test_diff = abs(test_dist - original_dist).max()
    
    if max_train_diff <= 0.01 and max_test_diff <= 0.01:
        print(f"   ‚úÖ Stratification excellente (√©carts < 1%)")
    elif max_train_diff <= 0.02 and max_test_diff <= 0.02:
        print(f"   ‚úÖ Stratification tr√®s bonne (√©carts < 2%)")
    else:
        print(f"   ‚ö†Ô∏è  Stratification acceptable (√©carts max: {max(max_train_diff, max_test_diff):.1%})")
    
    # V√âRIFICATION CRITIQUE: Absence de fuites par construction
    print(f"\nüîí V√©rification de l'absence de fuites:")
    
    # IDs
    train_ids = set(train_df['id'])
    test_ids = set(test_df['id'])
    common_ids = train_ids.intersection(test_ids)
    print(f"   IDs en commun: {len(common_ids)}")
    
    # Textes
    train_texts = set(train_df['text'].str.strip().str.lower())
    test_texts = set(test_df['text'].str.strip().str.lower())
    common_texts = train_texts.intersection(test_texts)
    print(f"   Textes en commun: {len(common_texts)}")
    
    if len(common_ids) == 0 and len(common_texts) == 0:
        print(f"   ‚úÖ PARFAIT: Aucune fuite d√©tect√©e par construction!")
        leak_status = "AUCUNE"
    else:
        print(f"   ‚ùå PROBL√àME: Fuites d√©tect√©es malgr√© le partitionnement!")
        leak_status = "D√âTECT√âE"
    
    # Sauvegarde des nouvelles partitions
    print(f"\nüíæ Sauvegarde des nouvelles partitions:")
    
    # Backup des anciennes partitions si elles existent
    old_train_path = 'data/raw/train_tweets.csv'
    old_test_path = 'data/raw/test_tweets.csv'
    
    if os.path.exists(old_train_path):
        backup_train = 'data/raw/train_tweets_old.csv'
        os.rename(old_train_path, backup_train)
        print(f"   Backup ancien train: {backup_train}")
    
    if os.path.exists(old_test_path):
        backup_test = 'data/raw/test_tweets_old.csv'
        os.rename(old_test_path, backup_test)
        print(f"   Backup ancien test: {backup_test}")
    
    # Sauvegarde des nouvelles partitions
    train_df.to_csv(old_train_path, index=False)
    test_df.to_csv(old_test_path, index=False)
    
    print(f"   ‚úÖ Nouveau train: {old_train_path}")
    print(f"   ‚úÖ Nouveau test: {old_test_path}")
    
    # Rapport final
    print(f"\n" + "=" * 50)
    print(f"üìã RAPPORT DE PARTITIONNEMENT")
    print(f"=" * 50)
    print(f"üìä Dataset source: {len(df_cleaned)} √©chantillons")
    print(f"üöÇ Partition train: {len(train_df)} √©chantillons ({len(train_df)/len(df_cleaned)*100:.1f}%)")
    print(f"üß™ Partition test: {len(test_df)} √©chantillons ({len(test_df)/len(df_cleaned)*100:.1f}%)")
    print(f"‚öñÔ∏è  √âcart de stratification max: {max(max_train_diff, max_test_diff):.2%}")
    print(f"üîí Fuites de donn√©es: {leak_status}")
    print(f"‚úÖ Qualit√©: {'EXCELLENTE' if leak_status == 'AUCUNE' else 'PROBL√âMATIQUE'}")
    print(f"=" * 50)
    
    return True

if __name__ == "__main__":
    success = create_clean_split()
    if success:
        print(f"\nüéâ Partitionnement termin√© avec succ√®s!")
        print(f"‚û°Ô∏è  Pr√™t pour le preprocessing des nouvelles partitions")
    else:
        print(f"\n‚ùå Erreur lors du partitionnement")
