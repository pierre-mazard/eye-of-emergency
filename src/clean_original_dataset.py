"""
Script de nettoyage du dataset original
1. Analyse des doublons dans original_train_tweets.csv
2. Suppression des doublons exacts
3. Sauvegarde du fichier nettoyÃ©
"""

import pandas as pd
import numpy as np
import os
from collections import Counter

def analyze_and_clean_original_dataset():
    """
    Analyse et nettoie le dataset original des doublons
    """
    print("ðŸ” ANALYSE ET NETTOYAGE DU DATASET ORIGINAL")
    print("=" * 60)
    
    # Chargement du dataset original
    original_path = 'data/raw/original_train_tweets.csv'
    print(f"ðŸ“ Chargement de {original_path}")
    
    try:
        df_original = pd.read_csv(original_path)
        print(f"âœ… Dataset chargÃ©: {len(df_original)} Ã©chantillons")
    except FileNotFoundError:
        print(f"âŒ ERREUR: Fichier {original_path} non trouvÃ©")
        return False
    
    print(f"ðŸ“Š Colonnes: {list(df_original.columns)}")
    print(f"ðŸ“Š Shape: {df_original.shape}")
    
    # Analyse des valeurs manquantes
    print(f"\nðŸ“‹ Analyse des valeurs manquantes:")
    missing_analysis = df_original.isnull().sum()
    for col, missing_count in missing_analysis.items():
        if missing_count > 0:
            pct = (missing_count / len(df_original)) * 100
            print(f"   {col}: {missing_count} ({pct:.1f}%)")
        else:
            print(f"   {col}: aucune valeur manquante")
    
    # Analyse de la distribution des classes AVANT nettoyage
    print(f"\nðŸ“ˆ Distribution des classes AVANT nettoyage:")
    class_dist_before = df_original['target'].value_counts().sort_index()
    for class_label, count in class_dist_before.items():
        pct = (count / len(df_original)) * 100
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        print(f"   Classe {class_label} ({class_name}): {count:,} ({pct:.1f}%)")
    
    # 1. ANALYSE DES DOUBLONS EXACTS (toutes colonnes)
    print(f"\nðŸ” ANALYSE DES DOUBLONS COMPLETS...")
    duplicates_all = df_original.duplicated()
    duplicate_count_all = duplicates_all.sum()
    print(f"   Doublons complets (toutes colonnes): {duplicate_count_all}")
    
    # 2. ANALYSE DES DOUBLONS DE TEXTE
    print(f"\nðŸ” ANALYSE DES DOUBLONS DE TEXTE...")
    
    # Doublons exacts de texte
    text_duplicates_exact = df_original['text'].duplicated()
    text_duplicate_count_exact = text_duplicates_exact.sum()
    print(f"   Doublons de texte exacts: {text_duplicate_count_exact}")
    
    # Doublons de texte (case-insensitive et stripped)
    df_original['text_normalized'] = df_original['text'].str.strip().str.lower()
    text_duplicates_normalized = df_original['text_normalized'].duplicated()
    text_duplicate_count_normalized = text_duplicates_normalized.sum()
    print(f"   Doublons de texte normalisÃ©s: {text_duplicate_count_normalized}")
    
    # Analyse des groupes de doublons
    if text_duplicate_count_normalized > 0:
        print(f"\nðŸ“Š ANALYSE DES GROUPES DE DOUBLONS:")
        duplicate_groups = df_original[df_original['text_normalized'].duplicated(keep=False)]
        group_sizes = duplicate_groups.groupby('text_normalized').size()
        
        print(f"   Nombre de groupes de doublons: {len(group_sizes)}")
        print(f"   Taille moyenne des groupes: {group_sizes.mean():.1f}")
        print(f"   Taille max des groupes: {group_sizes.max()}")
        
        # Exemples de doublons
        print(f"\nðŸ“ Exemples de textes dupliquÃ©s:")
        largest_groups = group_sizes.nlargest(3)
        for i, (text_norm, size) in enumerate(largest_groups.items(), 1):
            original_texts = df_original[df_original['text_normalized'] == text_norm]['text'].unique()
            preview = original_texts[0][:80] + "..." if len(original_texts[0]) > 80 else original_texts[0]
            print(f"   {i}. Groupe de {size} doublons: '{preview}'")
    
    # 3. ANALYSE DE L'IMPACT SUR LES CLASSES
    if text_duplicate_count_normalized > 0:
        print(f"\nâš–ï¸ IMPACT DES DOUBLONS SUR LES CLASSES:")
        
        # VÃ©rifier si les doublons ont le mÃªme label
        duplicate_mask = df_original['text_normalized'].duplicated(keep=False)
        duplicate_data = df_original[duplicate_mask]
        
        # Grouper par texte et vÃ©rifier la cohÃ©rence des labels
        label_consistency = duplicate_data.groupby('text_normalized')['target'].nunique()
        inconsistent_labels = (label_consistency > 1).sum()
        
        if inconsistent_labels > 0:
            print(f"   âš ï¸  ATTENTION: {inconsistent_labels} groupes ont des labels incohÃ©rents!")
            print(f"   ðŸ“‹ Exemples de labels incohÃ©rents:")
            inconsistent_groups = label_consistency[label_consistency > 1].head(3)
            for text_norm in inconsistent_groups.index:
                group_data = duplicate_data[duplicate_data['text_normalized'] == text_norm]
                labels = group_data['target'].unique()
                preview = group_data['text'].iloc[0][:60] + "..."
                print(f"      '{preview}' -> Labels: {labels}")
        else:
            print(f"   âœ… Tous les doublons ont des labels cohÃ©rents")
    
    # 4. NETTOYAGE DES DOUBLONS
    print(f"\nðŸ§¹ NETTOYAGE DES DOUBLONS...")
    
    # StratÃ©gie: Garder le premier exemplaire de chaque texte normalisÃ©
    df_cleaned = df_original.drop_duplicates(subset=['text_normalized'], keep='first')
    removed_count = len(df_original) - len(df_cleaned)
    
    print(f"   Ã‰chantillons supprimÃ©s: {removed_count}")
    print(f"   Dataset nettoyÃ©: {len(df_cleaned)} Ã©chantillons")
    
    # Suppression de la colonne temporaire
    df_cleaned = df_cleaned.drop('text_normalized', axis=1)
    
    # VÃ©rification de la distribution des classes APRÃˆS nettoyage
    print(f"\nðŸ“ˆ Distribution des classes APRÃˆS nettoyage:")
    class_dist_after = df_cleaned['target'].value_counts().sort_index()
    for class_label, count in class_dist_after.items():
        pct = (count / len(df_cleaned)) * 100
        class_name = "Non-catastrophe" if class_label == 0 else "Catastrophe"
        before_count = class_dist_before[class_label]
        change = count - before_count
        print(f"   Classe {class_label} ({class_name}): {count:,} ({pct:.1f}%) [Î”{change:+d}]")
    
    # Calcul des Ã©carts de distribution
    pct_before = class_dist_before / len(df_original) * 100
    pct_after = class_dist_after / len(df_cleaned) * 100
    max_change = abs(pct_after - pct_before).max()
    
    if max_change <= 1.0:
        print(f"   âœ… Distribution bien prÃ©servÃ©e (Ã©cart max: {max_change:.2f}%)")
    elif max_change <= 3.0:
        print(f"   âš ï¸  Distribution lÃ©gÃ¨rement modifiÃ©e (Ã©cart max: {max_change:.2f}%)")
    else:
        print(f"   âŒ Distribution significativement modifiÃ©e (Ã©cart max: {max_change:.2f}%)")
    
    # 5. SAUVEGARDE
    print(f"\nðŸ’¾ SAUVEGARDE...")
    
    # Backup de l'original
    backup_path = 'data/raw/original_train_tweets_with_duplicates.csv'
    if not os.path.exists(backup_path):
        df_original.drop('text_normalized', axis=1).to_csv(backup_path, index=False)
        print(f"   Backup de l'original: {backup_path}")
    
    # Sauvegarde du fichier nettoyÃ©
    cleaned_path = 'data/raw/original_train_tweets_cleaned.csv'
    df_cleaned.to_csv(cleaned_path, index=False)
    print(f"   Dataset nettoyÃ©: {cleaned_path}")
    
    # 6. RAPPORT FINAL
    print(f"\n" + "=" * 60)
    print(f"ðŸ“‹ RAPPORT DE NETTOYAGE")
    print(f"=" * 60)
    print(f"ðŸ“Š Dataset original: {len(df_original)} Ã©chantillons")
    print(f"ðŸ—‘ï¸  Doublons supprimÃ©s: {removed_count}")
    print(f"âœ¨ Dataset nettoyÃ©: {len(df_cleaned)} Ã©chantillons")
    print(f"ðŸ“‰ RÃ©duction: {(removed_count/len(df_original)*100):.1f}%")
    print(f"âš–ï¸  Ã‰cart de distribution max: {max_change:.2f}%")
    print(f"âœ… Fichier de sortie: {cleaned_path}")
    print(f"=" * 60)
    
    return True, cleaned_path, len(df_cleaned)

if __name__ == "__main__":
    success, output_path, final_count = analyze_and_clean_original_dataset()
    if success:
        print(f"\nðŸŽ‰ Nettoyage terminÃ© avec succÃ¨s!")
        print(f"âž¡ï¸  Fichier nettoyÃ© prÃªt pour le partitionnement: {output_path}")
        print(f"ðŸ“Š {final_count} Ã©chantillons uniques disponibles")
    else:
        print(f"\nâŒ Erreur lors du nettoyage")
