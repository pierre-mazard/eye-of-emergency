# Structure du dossier src/

Ce dossier contient tous les modules Python du projet Eye of Emergency.

## ğŸ“‹ PrÃ©requis

Avant d'utiliser les modules de ce dossier :

1. **Installation des dÃ©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

2. **Fichier requis** : Assurez-vous que `data/raw/original_train_tweets.csv` existe

3. **Structure des dossiers** : Les dossiers `data/processed/` seront crÃ©Ã©s automatiquement

## Fichiers principaux

### `pipeline.py` ğŸš€
**Fichier principal** - Pipeline complet intÃ©grÃ©
- ExÃ©cute toutes les Ã©tapes dans le bon ordre
- UtilisÃ© par `run_pipeline.py` (depuis la racine du projet)
- **RECOMMANDÃ‰** : Utiliser ce pipeline pour toute nouvelle exÃ©cution
- Gestion automatique des erreurs et validation Ã  chaque Ã©tape

### `preprocess_train.py` ğŸ”§
Module de preprocessing optimisÃ© pour l'entraÃ®nement
- Classe `OptimizedEmergencyPreprocessor` avec 16 features optimisÃ©es
- Feature engineering avancÃ© et nettoyage textuel intelligent
- BasÃ© sur l'analyse de validation V3.1 pour maximiser le pouvoir prÃ©dictif
- UtilisÃ© par le pipeline principal (Ã©tape 3)

### `preprocess_test.py` ğŸ§½
Module de preprocessing pour les donnÃ©es de test
- **Nettoyage uniquement** (pas de feature engineering pour Ã©viter les fuites)
- Applique la mÃªme mÃ©thode de nettoyage textuel que le train
- PrÃ©serve l'intÃ©gritÃ© des donnÃ©es de test
- UtilisÃ© par le pipeline principal (Ã©tape 4)

### `models.py` ğŸ¤–
DÃ©finitions des modÃ¨les de machine learning
- **En cours de dÃ©veloppement** pour la phase d'entraÃ®nement
- Contiendra les classes et fonctions des modÃ¨les ML optimisÃ©s

## Modules spÃ©cialisÃ©s (hÃ©ritÃ©s)

Les fichiers suivants ont Ã©tÃ© intÃ©grÃ©s dans `pipeline.py` mais sont conservÃ©s pour rÃ©fÃ©rence historique et maintenance :

### `clean_original_dataset.py` ğŸ§¹
- **Fonction** : Nettoyage du dataset original (suppression des doublons exacts)
- **Statut** : **INTÃ‰GRÃ‰** dans l'Ã©tape 1 du pipeline (`step1_clean_original_dataset`)
- **Usage** : Analyse et suppression des textes dupliquÃ©s avec prÃ©servation de la distribution des classes

### `create_clean_split.py` âš–ï¸
- **Fonction** : Partitionnement stratifiÃ© propre (90% train / 10% test)
- **Statut** : **INTÃ‰GRÃ‰** dans l'Ã©tape 2 du pipeline (`step2_create_clean_split`)
- **Usage** : Split stratifiÃ© avec vÃ©rification automatique de l'absence de fuites

### `clean_final_leakage.py` ğŸ”’
- **Fonction** : Nettoyage final des fuites de donnÃ©es aprÃ¨s preprocessing
- **Statut** : **INTÃ‰GRÃ‰** dans l'Ã©tape 5 du pipeline (`step5_clean_final_leakage`)
- **Usage** : Suppression des textes en commun entre train et test finaux
- **Avantage** : Version optimisÃ©e et plus rÃ©cente

### `clean_data_leakage.py` âš ï¸
- **Fonction** : Version alternative de nettoyage des fuites
- **Statut** : **REMPLACÃ‰** par `clean_final_leakage.py` (plus efficace)
- **Usage** : ConservÃ© pour rÃ©fÃ©rence historique uniquement

## ğŸ“ Fichiers systÃ¨me

### `__pycache__/` ğŸ—‚ï¸
- **Contenu** : Cache Python gÃ©nÃ©rÃ© automatiquement
- **Statut** : IgnorÃ© par Git (voir `.gitignore`)
- **Maintenance** : Peut Ãªtre supprimÃ© sans risque (`rm -rf __pycache__`)

### `.gitignore` ğŸš«
- **Fonction** : Configuration Git pour ignorer les fichiers temporaires
- **Contenu** : Cache Python (*.pyc, __pycache__/), fichiers temporaires

## Usage recommandÃ©

### ğŸš€ ExÃ©cution du pipeline complet (RECOMMANDÃ‰)

Pour exÃ©cuter le pipeline complet depuis la racine du projet :

```bash
# Depuis la racine du projet eye-of-emergency/
python run_pipeline.py
```

**Le pipeline exÃ©cute automatiquement dans l'ordre :**

1. ğŸ§¹ **Nettoyage du dataset original** (`step1_clean_original_dataset`)
   - Suppression des doublons exacts et normalisÃ©s
   - PrÃ©servation de la distribution des classes
   - GÃ©nÃ©ration de `original_train_tweets_cleaned.csv`

2. âš–ï¸ **Partitionnement stratifiÃ©** (`step2_create_clean_split`)
   - Split 90% train / 10% test avec stratification
   - VÃ©rification automatique de l'absence de fuites
   - GÃ©nÃ©ration de `train_tweets.csv` et `test_tweets.csv`

3. ğŸ”§ **Preprocessing train** (`step3_preprocess_train`)
   - Feature engineering avec 16 features optimisÃ©es
   - Nettoyage textuel intelligent et extraction de patterns
   - GÃ©nÃ©ration de `train_optimized.csv`

4. ğŸ§½ **Preprocessing test** (`step4_preprocess_test`)
   - Nettoyage textuel uniquement (pas de feature engineering)
   - PrÃ©servation de l'intÃ©gritÃ© des donnÃ©es de test
   - GÃ©nÃ©ration de `test_cleaned.csv`

5. ğŸ—‘ï¸ **Nettoyage final des fuites** (`step5_clean_final_leakage`)
   - DÃ©tection et suppression des textes communs rÃ©siduels
   - VÃ©rification finale de l'intÃ©gritÃ© train/test

6. âœ… **Validation de l'intÃ©gritÃ©** (`step6_validate_integrity`)
   - Score de qualitÃ© final (objectif : >85/100)
   - Rapport complet de validation

### ğŸ”§ ExÃ©cution modulaire (pour dÃ©veloppement)

Pour exÃ©cuter des modules individuellement depuis le dossier `src/` :

```bash
# Exemples d'utilisation modulaire
cd src/

# Nettoyage uniquement du dataset original
python clean_original_dataset.py

# Preprocessing du train uniquement  
python preprocess_train.py

# Nettoyage des fuites uniquement
python clean_final_leakage.py
```

âš ï¸ **Attention** : L'exÃ©cution modulaire nÃ©cessite que les Ã©tapes prÃ©cÃ©dentes aient Ã©tÃ© rÃ©alisÃ©es.

## Avantages du pipeline intÃ©grÃ©

- âœ… **Robuste** : Gestion automatique des erreurs Ã  chaque Ã©tape avec rollback
- âœ… **Reproductible** : Toujours le mÃªme ordre d'exÃ©cution avec graines fixes (random_state=42)
- âœ… **ValidÃ©** : Score d'intÃ©gritÃ© finale de 100/100 garanti
- âœ… **Sans fuites** : VÃ©rification automatique et suppression des fuites Ã  chaque Ã©tape
- âœ… **OptimisÃ©** : BasÃ© sur l'analyse de validation V3.1 avec 16 features sÃ©lectionnÃ©es
- âœ… **TraÃ§able** : Logs dÃ©taillÃ©s et rapports de progression Ã  chaque Ã©tape
- âœ… **SÃ©curisÃ©** : Backup automatique des fichiers avant modification

## Structure de sortie

### ğŸ“ Fichiers gÃ©nÃ©rÃ©s dans `data/processed/`

| Fichier | Description | Taille estimÃ©e | Usage |
|---------|-------------|----------------|-------|
| `train_optimized.csv` | Dataset d'entraÃ®nement avec 16 features optimisÃ©es | ~6,000-6,500 Ã©chantillons | EntraÃ®nement ML |
| `test_cleaned.csv` | Dataset de test nettoyÃ© (sans features) | ~700-750 Ã©chantillons | Ã‰valuation finale |

### ğŸ“ Fichiers de sauvegarde dans `data/raw/`

| Fichier | Description | Statut |
|---------|-------------|---------|
| `original_train_tweets_cleaned.csv` | Dataset original sans doublons | GÃ©nÃ©rÃ© par Ã©tape 1 |
| `original_train_tweets_with_duplicates.csv` | Backup de l'original | Backup automatique |
| `train_tweets.csv` | Partition train aprÃ¨s split | GÃ©nÃ©rÃ© par Ã©tape 2 |
| `test_tweets.csv` | Partition test aprÃ¨s split | GÃ©nÃ©rÃ© par Ã©tape 2 |

## ğŸ” Diagnostic et maintenance

### VÃ©rification de l'Ã©tat du pipeline

```bash
# VÃ©rifier les fichiers gÃ©nÃ©rÃ©s
ls -la data/processed/
ls -la data/raw/

# VÃ©rifier la qualitÃ© des donnÃ©es (depuis Python)
python -c "
import pandas as pd
train = pd.read_csv('data/processed/train_optimized.csv')
test = pd.read_csv('data/processed/test_cleaned.csv')
print(f'Train: {len(train)} Ã©chantillons, {len(train.columns)} colonnes')
print(f'Test: {len(test)} Ã©chantillons, {len(test.columns)} colonnes')
print(f'Features communes: {set(train.columns) & set(test.columns)}')
"
```

### Nettoyage et redÃ©marrage

```bash
# Nettoyer le cache Python
find . -name "__pycache__" -type d -exec rm -rf {} +
find . -name "*.pyc" -delete

# RedÃ©marrer le pipeline depuis zÃ©ro (ATTENTION: supprime tous les fichiers gÃ©nÃ©rÃ©s)
rm -f data/processed/*.csv
rm -f data/raw/*_cleaned.csv data/raw/*_with_*.csv
python run_pipeline.py
```

## ğŸ“Š MÃ©triques de performance attendues

AprÃ¨s exÃ©cution complÃ¨te du pipeline :

- **Score de qualitÃ©** : >85/100 (objectif validÃ©)
- **IntÃ©gritÃ© des donnÃ©es** : 100% (0 fuite garantie)
- **RÃ©duction des donnÃ©es** : ~15-20% (suppression doublons + nettoyage)
- **Features finales** : 16 features optimisÃ©es pour ML
- **Temps d'exÃ©cution** : ~2-5 minutes selon la machine

---

ğŸ’¡ **Conseil** : Utilisez toujours `run_pipeline.py` pour garantir un workflow complet et validÃ© !
