# Eye of Emergency ğŸš¨

DÃ©veloppement d'un modÃ¨le d'apprentissage automatique capable de classer des tweets signalant des catastrophes naturelles rÃ©elles pour aider les intervenants d'urgence et le public Ã  accÃ©der Ã  des informations prÃ©cises et fiables en pÃ©riode de crise.

## ğŸ“‹ Table des matiÃ¨res

- [Contexte du projet](#contexte-du-projet)
- [DonnÃ©es](#donnÃ©es)
- [Veille NLP](#veille-nlp)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Structure du projet](#structure-du-projet)
- [MÃ©thodologie](#mÃ©thodologie)
- [Algorithmes utilisÃ©s](#algorithmes-utilisÃ©s)
- [RÃ©sultats et analyse](#rÃ©sultats-et-analyse)
- [Conclusion](#conclusion)

## ğŸ¯ Contexte du projet

L'objectif de ce projet est de dÃ©velopper un modÃ¨le d'apprentissage automatique capable de distinguer les tweets informatifs concernant des catastrophes naturelles rÃ©elles de ceux qui ne le sont pas. Cette classification automatique permettra d'aider les services d'urgence Ã  identifier rapidement les informations fiables lors de situations de crise.

## ğŸ“Š DonnÃ©es

Le projet utilise le dataset **Disaster Tweets** qui contient les colonnes suivantes :

- **id** : Identifiant unique pour chaque tweet
- **text** : Contenu textuel du tweet
- **location** : Localisation d'oÃ¹ a Ã©tÃ© envoyÃ© le tweet
- **keyword** : Mot-clÃ© associÃ© au tweet
- **target** : Variable cible indiquant si un tweet concerne une catastrophe rÃ©elle (1) ou non (0)

## ğŸ§  Veille NLP

![BanniÃ¨re Veille NLP - Concepts fondamentaux du traitement automatique du langage naturel](./assets/images/veille_nlp_banner.webp)

Cette section prÃ©sente une veille technologique sur les concepts fondamentaux du traitement automatique du langage naturel (NLP) et du text mining. Cette recherche thÃ©orique constitue la base mÃ©thodologique pour comprendre les techniques utilisÃ©es dans ce projet de classification de tweets d'urgence.

### 1. Text Mining vs Natural Language Processing

- ğŸ“Š**Text Mining** : Processus d'extraction de connaissances et d'informations utiles Ã  partir de grandes quantitÃ©s de donnÃ©es textuelles non structurÃ©es. Il combine des techniques de data mining, d'apprentissage automatique et de statistiques pour dÃ©couvrir des patterns, tendances et insights cachÃ©s dans les textes.

<div align="center">
  <img src="./assets/images/diagram_text_mining.jpg" alt="Illustration du processus de Text Mining" width="700"/>
</div>

- ğŸ¤–**NLP (Natural Language Processing)** : Branche de l'intelligence artificielle qui permet aux machines de comprendre, interprÃ©ter et gÃ©nÃ©rer le langage humain de maniÃ¨re naturelle. Le NLP se concentre sur la comprÃ©hension du sens, de la syntaxe et du contexte linguistique.

<div align="center">
  <img src="./assets/images/diagram_npl.webp" alt="Illustration du processus du Natural Language Processing" width="700"/>
</div>

| **Aspects** | **Text Mining** ğŸ“Š | **Natural Language Processing** ğŸ¤– |
|--------------|-----------------|----------------------------------|
| **ğŸ¯ Objectif principal** | Extraction de patterns et d'insights Ã  partir de volumes importants de texte | ComprÃ©hension et traitement du langage humain par les machines |
| **ğŸ” Focus** | DÃ©couverte de connaissances, analyse de tendances | Analyse linguistique, comprÃ©hension du sens |
| **âš™ï¸ MÃ©thodes** | Clustering, classification, association de rÃ¨gles | Analyse syntaxique, sÃ©mantique, pragmatique |
| **ğŸ’¼ Applications** | Business intelligence, veille stratÃ©gique, analyse de sentiment Ã  grande Ã©chelle | Traduction automatique, chatbots, assistants vocaux |
| **ğŸ“„ DonnÃ©es** | Corpus de textes volumineux, documents non structurÃ©s | Phrases, dialogues, textes avec structure linguistique |
| **ğŸ“ˆ RÃ©sultats** | Rapports, visualisations, modÃ¨les prÃ©dictifs | Texte traitÃ©, entitÃ©s extraites, rÃ©ponses gÃ©nÃ©rÃ©es |

### 2. Sous-domaines du NLP
- **Analyse de sentiments** ğŸ˜ŠğŸ˜ğŸ˜¢ : Technique permettant d'identifier et d'extraire les opinions, Ã©motions et attitudes exprimÃ©es dans un texte. Elle classe les textes selon leur polaritÃ© (positif, nÃ©gatif, neutre) et peut dÃ©tecter des Ã©motions spÃ©cifiques (joie, colÃ¨re, peur, etc.). ParticuliÃ¨rement utile pour analyser les rÃ©actions du public face aux catastrophes.

- **Named Entity Recognition (NER)** ğŸ·ï¸ : Processus d'identification et de classification des entitÃ©s nommÃ©es dans un texte (personnes, lieux, organisations, dates, montants, etc.). Dans le contexte des tweets d'urgence, le NER peut extraire des informations cruciales comme les lieux de catastrophes, les organisations de secours impliquÃ©es, ou les dates d'Ã©vÃ©nements.

- **Part-of-Speech (POS) Tagging** ğŸ“ : Attribution d'une catÃ©gorie grammaticale (nom, verbe, adjectif, etc.) Ã  chaque mot d'un texte. Cette analyse syntaxique aide Ã  comprendre la structure grammaticale et peut amÃ©liorer la prÃ©cision d'autres tÃ¢ches NLP comme la classification de texte.

### 3. Applications concrÃ¨tes du NLP

Le NLP trouve de nombreuses applications dans notre quotidien et dans des domaines spÃ©cialisÃ©s :

- **ğŸ” Moteurs de recherche** : Google, Bing utilisent le NLP pour comprendre les requÃªtes utilisateur et fournir des rÃ©sultats pertinents
- **ğŸ¤– Assistants virtuels** : Siri, Alexa, Google Assistant comprennent et rÃ©pondent au langage naturel
- **ğŸŒ Traduction automatique** : Google Translate, DeepL permettent la traduction en temps rÃ©el
- **ğŸ“§ Filtrage de spam** : Classification automatique des emails indÃ©sirables
- **ğŸ’¬ Chatbots** : Service client automatisÃ©, support technique
- **ğŸ“° RÃ©sumÃ© automatique** : GÃ©nÃ©ration de rÃ©sumÃ©s d'articles ou de documents
- **ğŸš¨ DÃ©tection de fake news** : Identification de fausses informations
- **ğŸ“Š Analyse de rÃ©seaux sociaux** : Monitoring de l'opinion publique, dÃ©tection de tendances
- **âš•ï¸ Analyse mÃ©dicale** : Traitement de dossiers patients, aide au diagnostic

### 4. Stop-words
- **DÃ©finition** ğŸ›‘ : Mots trÃ¨s frÃ©quents dans une langue qui n'apportent gÃ©nÃ©ralement pas d'information sÃ©mantique significative pour l'analyse textuelle. Ces mots de liaison, articles, prÃ©positions et pronoms sont souvent filtrÃ©s lors du prÃ©processing pour se concentrer sur les mots porteurs de sens.

- **Importance de leur suppression** âœ‚ï¸ : 
  - **RÃ©duction du bruit** : Ã‰limination des mots non informatifs qui peuvent masquer les patterns importants
  - **AmÃ©lioration des performances** : RÃ©duction de la dimensionnalitÃ© et accÃ©lÃ©ration des calculs
  - **Focus sur le contenu** : Concentration sur les mots clÃ©s porteurs de sens
  - **Optimisation mÃ©moire** : Diminution de l'espace de stockage nÃ©cessaire

- **Exemples** ğŸ“ :
  - **FranÃ§ais** : le, la, les, de, du, des, et, ou, Ã , dans, pour, sur, avec, sans, Ãªtre, avoir, faire...
  - **Anglais** : the, a, an, and, or, but, in, on, at, to, for, of, with, by, from, is, are, was, were...
  - **Contexte spÃ©cifique** : Dans l'analyse de tweets, on peut aussi considÃ©rer comme stop-words : "RT", "@", "http", emojis selon le contexte

### 5. Traitement des caractÃ¨res spÃ©ciaux et ponctuation

Le nettoyage des caractÃ¨res spÃ©ciaux et de la ponctuation est une Ã©tape cruciale du prÃ©processing textuel :

**ğŸ”§ Techniques principales :**
- **Suppression de la ponctuation** : Ã‰limination des signes de ponctuation (.,!?;:) qui peuvent crÃ©er du bruit
- **Normalisation des caractÃ¨res** : Conversion des caractÃ¨res accentuÃ©s (Ã© â†’ e, Ã  â†’ a) pour uniformiser
- **Gestion des caractÃ¨res spÃ©ciaux** : Traitement des symboles (@, #, $, %, &) selon le contexte
- **Nettoyage des espaces** : Suppression des espaces multiples, tabulations, retours Ã  la ligne

**ğŸ“± SpÃ©cificitÃ©s des rÃ©seaux sociaux :**
- **Hashtags** : Conservation ou suppression du # selon l'analyse souhaitÃ©e
- **Mentions** : Traitement des @ mentions (@utilisateur)
- **URLs** : Suppression ou remplacement par un token gÃ©nÃ©rique
- **Emojis** : Conservation, suppression ou conversion en texte selon le besoin
- **CaractÃ¨res rÃ©pÃ©tÃ©s** : Normalisation ("wouuuuu" â†’ "wouu")

**âš–ï¸ Ã‰quilibre nÃ©cessaire :**
Il faut trouver le bon Ã©quilibre entre nettoyage et conservation de l'information utile. Par exemple, les points d'exclamation peuvent indiquer l'urgence dans les tweets de catastrophe.

### 6. Tokenisation et N-grams

- **Token** ğŸª™ : UnitÃ© linguistique de base obtenue aprÃ¨s dÃ©coupage d'un texte. Un token peut Ãªtre un mot, un caractÃ¨re, ou mÃªme une sous-partie de mot selon la mÃ©thode de tokenisation choisie. C'est l'Ã©lÃ©ment atomique manipulÃ© par les algorithmes de NLP.

- **N-gram** ğŸ”— : SÃ©quence contiguÃ« de N Ã©lÃ©ments (mots, caractÃ¨res, ou tokens) extraite d'un texte. Les n-grams capturent le contexte local et les relations entre mots adjacents :
  - **Unigram (1-gram)** : Mots individuels ["catastrophe", "naturelle"]
  - **Bigram (2-gram)** : Paires de mots ["catastrophe naturelle", "secours d'urgence"]
  - **Trigram (3-gram)** : Triplets de mots ["catastrophe naturelle majeure"]

- **Processus de tokenisation** âš™ï¸ :
  1. **Segmentation** : DÃ©coupage du texte brut en unitÃ©s plus petites
  2. **Normalisation** : Conversion en minuscules, suppression d'accents
  3. **Filtrage** : Suppression des stop-words, caractÃ¨res spÃ©ciaux
  4. **Validation** : VÃ©rification de la cohÃ©rence des tokens obtenus
  
  **Exemple** : "Les secours arrivent !" â†’ ["les", "secours", "arrivent"] â†’ ["secours", "arrivent"] (aprÃ¨s suppression stop-words)

### 7. Stemming vs Lemmatisation

- **Stemming** âœ‚ï¸ : Processus de rÃ©duction des mots Ã  leur racine (stem) en supprimant les suffixes et prÃ©fixes selon des rÃ¨gles algorithmiques. Rapide mais parfois imprÃ©cis, il peut produire des stems qui ne sont pas des mots rÃ©els.
  - **Exemple** : "courir", "courant", "couraient" â†’ "cour"

- **Lemmatisation** ğŸ¯ : Processus de rÃ©duction des mots Ã  leur forme canonique (lemme) en utilisant un dictionnaire et une analyse morphologique. Plus prÃ©cise que le stemming, elle produit toujours des mots existants.
  - **Exemple** : "courir", "courant", "couraient" â†’ "courir"

**ğŸ” DiffÃ©rences principales :**

| **Aspect** | **Stemming** | **Lemmatisation** |
|------------|--------------|-------------------|
| **ğŸš€ RapiditÃ©** | TrÃ¨s rapide | Plus lente |
| **ğŸ¯ PrÃ©cision** | Moins prÃ©cise | Plus prÃ©cise |
| **ğŸ“š Ressources** | RÃ¨gles algorithmiques | Dictionnaire + analyse grammaticale |
| **âœ… RÃ©sultat** | Stem (peut ne pas Ãªtre un mot) | Lemme (toujours un mot valide) |
| **ğŸ”§ ComplexitÃ©** | Simple | Complexe |

**ğŸª Cas d'usage :**
- **Stemming** : Recherche d'information, classification de texte oÃ¹ la vitesse prime
- **Lemmatisation** : Analyse sÃ©mantique fine, applications nÃ©cessitant une haute prÃ©cision
- **Projet Emergency** : Le stemming peut suffire pour la classification de tweets d'urgence oÃ¹ la rapiditÃ© est cruciale

### 8. MÃ©thodes de vectorisation

- **Bag of Words (BoW)** ğŸ’ : ReprÃ©sentation vectorielle qui compte la frÃ©quence d'apparition de chaque mot dans un document, sans tenir compte de l'ordre des mots. Chaque document devient un vecteur oÃ¹ chaque dimension correspond Ã  un mot du vocabulaire.
  
  **Exemple :**
  - Vocabulaire : ["incendie", "forÃªt", "secours", "urgent"]
  - Document : "Incendie de forÃªt urgent" â†’ [1, 1, 0, 1]
  
  **âœ… Avantages :** Simple, rapide, efficace pour la classification
  **âŒ InconvÃ©nients :** Perte du contexte, vecteurs trÃ¨s creux (sparse)

- **TF-IDF (Term Frequency-Inverse Document Frequency)** ğŸ“Š : MÃ©thode de pondÃ©ration qui mesure l'importance d'un mot dans un document relativement Ã  une collection de documents. Combine la frÃ©quence du terme (TF) avec l'inverse de sa frÃ©quence dans le corpus (IDF).

  **ğŸ“ Formules :**
  - **TF(t,d)** = (Nombre d'occurrences de t dans d) / (Nombre total de mots dans d)
  - **IDF(t,D)** = log(Nombre total de documents / Nombre de documents contenant t)
  - **TF-IDF(t,d,D)** = TF(t,d) Ã— IDF(t,D)

  **âœ… Avantages :** RÃ©duit l'impact des mots trÃ¨s frÃ©quents, met en valeur les mots discriminants
  **âŒ InconvÃ©nients :** Plus complexe, ne capture pas les relations sÃ©mantiques
  
  **ğŸ¯ Usage dans le projet :** TF-IDF sera particuliÃ¨rement utile pour identifier les mots-clÃ©s spÃ©cifiques aux tweets de catastrophe par rapport aux tweets normaux.

### 9. Bagging vs Boosting

- **Bagging (Bootstrap Aggregating)** ğŸ’ : Technique d'ensemble qui entraÃ®ne plusieurs modÃ¨les en parallÃ¨le sur des Ã©chantillons diffÃ©rents des donnÃ©es d'entraÃ®nement (avec remise). Les prÃ©dictions finales sont obtenues par vote majoritaire (classification) ou moyenne (rÃ©gression). RÃ©duit la variance et limite l'overfitting.
  
  **Exemples :** Random Forest, Extra Trees
  **Principe :** DiversitÃ© par Ã©chantillonnage des donnÃ©es

- **Boosting** ğŸš€ : Technique d'ensemble qui entraÃ®ne les modÃ¨les sÃ©quentiellement, oÃ¹ chaque nouveau modÃ¨le apprend des erreurs du prÃ©cÃ©dent. Les modÃ¨les faibles sont combinÃ©s pour former un modÃ¨le fort. RÃ©duit le biais et amÃ©liore la prÃ©cision.
  
  **Exemples :** AdaBoost, Gradient Boosting, XGBoost
  **Principe :** AmÃ©lioration itÃ©rative en se concentrant sur les erreurs

**ğŸ” Comparaison :**

| **Aspect** | **Bagging** ğŸ’ | **Boosting** ğŸš€ |
|------------|----------------|------------------|
| **ğŸ”„ EntraÃ®nement** | ParallÃ¨le (indÃ©pendant) | SÃ©quentiel (dÃ©pendant) |
| **ğŸ¯ Objectif** | RÃ©duire la variance | RÃ©duire le biais |
| **âš¡ Vitesse** | Plus rapide (parallÃ©lisable) | Plus lent (sÃ©quentiel) |
| **ğŸª Robustesse** | RÃ©sistant au bruit | Sensible au bruit/outliers |
| **ğŸ“Š Performance** | Stable, bonne gÃ©nÃ©ralisation | TrÃ¨s haute performance si bien calibrÃ© |
| **ğŸ”§ ComplexitÃ©** | Simple Ã  implÃ©menter | Plus complexe |

**ğŸ¯ Usage dans le projet :** Random Forest (bagging) pour la robustesse et XGBoost (boosting) pour la performance maximale seront comparÃ©s dans ce projet.

## ğŸ”§ PrÃ©requis

- Python 3.7+
- Jupyter Notebook
- BibliothÃ¨ques Python : pandas, numpy, scikit-learn, nltk, matplotlib, seaborn ... 

## ğŸš€ Installation

```bash
# Cloner le repository
git clone https://github.com/pierre-mazard/eye-of-emergency.git
cd eye-of-emergency

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer Jupyter Notebook
jupyter notebook
```

## ğŸ“ Structure du projet

```
eye-of-emergency/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ images/                        # Images pour la documentation (README, etc.)
â”‚   â””â”€â”€ README.md                      # Documentation des ressources visuelles
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ disaster_tweets.csv        # Dataset brut original
â”‚   â”œâ”€â”€ train.csv                      # Dataset d'entraÃ®nement prÃ©processÃ©
â”‚   â”œâ”€â”€ test.csv                       # Dataset de test prÃ©processÃ©
â”‚   â””â”€â”€ README.md                      # Documentation du dossier data
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eye_of_emergency_analysis.ipynb 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â””â”€â”€ README.md                  # Documentation des visualisations
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ README.md                  # Documentation des modÃ¨les sauvegardÃ©s
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ” MÃ©thodologie

### 1. Veille NLP
Ã‰tude des concepts fondamentaux du traitement du langage naturel

### 2. Exploration des donnÃ©es
- Analyse des donnÃ©es manquantes et doublons
- Visualisations spÃ©cifiques au NLP (nuages de mots, distribution des longueurs, etc.)

### 3. PrÃ©processing
- Nettoyage des donnÃ©es textuelles
- Pipeline de preprocessing personnalisÃ©

### 4. ModÃ©lisation
- DÃ©veloppement et comparaison de 5 modÃ¨les
- Optimisation par GridSearch

### 5. Ã‰valuation
- MÃ©triques d'Ã©valuation avec optimisation du F1-score

## ğŸ¤– Algorithmes utilisÃ©s

### ModÃ¨les de classification Ã©tudiÃ©s

1. **RÃ©gression Logistique**
   - [Ã€ complÃ©ter]

2. **Decision Tree**
   - ImplÃ©mentation d'une classe Python personnalisÃ©e
   - [Ã€ complÃ©ter]

3. **Random Forest**
   - [Ã€ complÃ©ter]

4. **XGBoost**
   - [Ã€ complÃ©ter]

5. **Support Vector Machine (SVM)**
   - [Ã€ complÃ©ter]

## ğŸ“ˆ RÃ©sultats et analyse

### Analyse exploratoire des donnÃ©es
- **DonnÃ©es manquantes** : [Ã€ complÃ©ter]
- **Doublons** : [Ã€ complÃ©ter]
- **Visualisations NLP** : [Ã€ complÃ©ter - nuages de mots, distribution des longueurs, etc.]

### Performance des modÃ¨les
- **Meilleur modÃ¨le** : [Ã€ complÃ©ter]
- **MÃ©triques d'Ã©valuation** :
  - **Accuracy** : [Ã€ complÃ©ter]
  - **Precision** : [Ã€ complÃ©ter]
  - **Recall** : [Ã€ complÃ©ter]
  - **F1-score** : [Ã€ complÃ©ter]
- **Matrice de confusion** : [Ã€ complÃ©ter]
- **Rapport de classification** : [Ã€ complÃ©ter]

## ğŸ¯ Conclusion

### Comparaison des modÃ¨les
[Ã€ complÃ©ter - analyse comparative des 5 modÃ¨les]

### ModÃ¨le sÃ©lectionnÃ©
[Ã€ complÃ©ter - justification du choix final]

### EfficacitÃ© et performance
[Ã€ complÃ©ter - Ã©valuation de l'efficacitÃ© du modÃ¨le choisi] 

