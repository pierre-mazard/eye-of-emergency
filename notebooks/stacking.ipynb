{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b534f",
   "metadata": {},
   "source": [
    "### Initialisation et test des différents models individuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81176891",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pass\n",
    "y_train = pass\n",
    "\n",
    "model_sgd = SGDClassifier(max_iter=1000) \n",
    "model_tree = DecisionTreeClassifier(max_depth=10)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "for model in (model_sgd, model_tree, model_knn):\n",
    "      model.fit(X_train, y_train)\n",
    "      print(f\"{model.__class__.__name__} score: {model.score(X_test, y_test)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90f794",
   "metadata": {},
   "source": [
    "### Techninque d'ensemble `VotingClassifier`\n",
    "\n",
    "- ``voting soft`` = prend la probabilité de chaque classe et fait la moyenne\n",
    "- ``voting hard`` = sélectionne la classe avec le plus de votes\n",
    "\n",
    "Vote soft un peu meilleur quand les models sont bien qualibrés, à utiliser quand on a des models qui sortent des probabilités\n",
    "\n",
    "`VotingClassifier` n'est pas une technique d'ensemble très efficace, elle ne garantie pas le respect du critère de diversité de la foule<br>\n",
    "\n",
    "**loi des grands nombe** :\n",
    "- taille\n",
    "- compétence\n",
    "- diversite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01700a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = SGDClassifier(max_iter=1000) \n",
    "model_tree = DecisionTreeClassifier(max_depth=10)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_voting = VotingClassifier([('SGD', model_sgd),\n",
    "                                 ['Tree', model_tree],\n",
    "                                 ['Knn', model_knn]],\n",
    "                                 voting='soft')\n",
    "\n",
    "for model in (model_sgd, model_tree, model_knn, model_voting):\n",
    "      model.fit(X_train, y_train)\n",
    "      print(f\"{model.__class__.__name__} score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555516c6",
   "metadata": {},
   "source": [
    "Pour gagner de la diversité, il faut utiliser les techniques de **bagging** ou **boosting**\n",
    "\n",
    "### Techninque d'ensemble Bagging\n",
    "A privilégier quand les models individuels font de l'overfitting\n",
    "\n",
    "Diférents models sont disponible : de regression, extraTree, ...\n",
    "\n",
    "Il faut déterminer :\n",
    "- un estimateur de base avec lequel travailler\n",
    "- le nombre d'estimateur dans la foule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072a934",
   "metadata": {},
   "source": [
    "\n",
    "### Bagging avec KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "model = BaggingClassifier(base_estimator = KNeighborsClassifier(), \n",
    "                          n_estimators= 100)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2f12f",
   "metadata": {},
   "source": [
    "### Bagging avec RandomForest\n",
    "meilleur résultat que Kneighbors, mais il faut le régler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Random forest utilise déjà par défaut un base_estimator de type arbre de décision\n",
    "model = RandomForestClassifier(n_estimators= 100, max_depth = 10)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02df68",
   "metadata": {},
   "source": [
    "### Technique d'ensemble Boosting\n",
    "\n",
    "A privilégier quand les models individuels ont de maucais résultats sur le train set, si le dataset est complexe, underfitting.\n",
    "\n",
    "Faire une GridSearchCV et tester des `base_estimators`, `learning_rate` et autres paramètres différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec12b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators= 100)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5628a",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "entraine un estimateur par dessus les pred des différents models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "meta_model = StackingClassifier([('SGD', model_sgd),\n",
    "                                 ('Tree', model_tree),\n",
    "                                 ('Knn', model_knn)],\n",
    "                                 final_estimator = KNeighborsClassifier())\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
